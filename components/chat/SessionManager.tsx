import { useState, useEffect, useRef } from 'react';
import { SessionManagerProps } from './types';
import { RagChunk } from '../../types';
import { generateEmbedding, cosineSimilarity, rerankChunks } from '../../services/embeddingService';
import { ragCacheManager } from '../../services/ragCacheManager';
import { searchSimilarChunks } from '../../services/tursoService';
import { streamChat } from '../../services/llmService';

const useSessionManager = ({ session, onSessionUpdate }: SessionManagerProps) => {
  const [currentSession, setCurrentSession] = useState(session);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    setCurrentSession(session);
  }, [session]);

  useEffect(() => {
    scrollToBottom();
  }, [currentSession.messages]);

  const findRelevantContext = async (
    message: string,
    assistantId: string,
    ragChunks: RagChunk[],
    setStatusText: (text: string) => void,
  ): Promise<string> => {
    try {
      console.log(`ğŸ¯ [SESSION RAG] Starting cached context search for query: "${message}"`);
      setStatusText('ğŸ” æª¢æŸ¥æŸ¥è©¢ç·©å­˜...');

      // Use cached RAG manager for the query
      const cacheResult = await ragCacheManager.performCachedRagQuery(
        message,
        assistantId,
        ragChunks,
        {
          similarityThreshold: 0.9, // High threshold for cache hits
          rerankLimit: 5, // SessionManager uses fixed limit of 5
          enableReranking: true, // Always enabled in SessionManager
          enableCache: true,
        },
      );

      const { results, fromCache, queryTime, cacheStats } = cacheResult;

      if (fromCache && cacheStats) {
        setStatusText(`âœ¨ ç·©å­˜å‘½ä¸­ï¼ç›¸ä¼¼åº¦: ${(cacheStats.similarity || 0).toFixed(3)}`);
        console.log(
          `ğŸ¯ [SESSION CACHE HIT] Query "${message}" matched "${cacheStats.originalQuery}" with similarity ${(cacheStats.similarity || 0).toFixed(4)} in ${queryTime}ms`,
        );
      } else {
        setStatusText(`ğŸ’¾ å®Œæ•´ RAG æŸ¥è©¢å®Œæˆ (${queryTime}ms)`);
        console.log(`ğŸ“Š [SESSION CACHE MISS] Performed full RAG query in ${queryTime}ms`);
      }

      // Convert results to context string
      if (results.length === 0) {
        console.log('âŒ [SESSION RAG] No relevant context found');
        return '';
      }

      const contextString = results
        .map(chunk => `From ${chunk.fileName}:\n${chunk.content}`)
        .join('\n\n---\n\n');

      console.log(
        `ğŸ“ [SESSION RAG] Final context: ${results.length} chunks, ${contextString.length} characters`,
      );
      console.log(`ğŸ“ˆ [SESSION CACHE STATS] From cache: ${fromCache}, Query time: ${queryTime}ms`);

      return contextString;
    } catch (error) {
      console.error('âŒ [SESSION RAG] Error in cached context search:', error);

      // Fallback to original implementation on cache error
      try {
        setStatusText('ğŸ”„ ç·©å­˜éŒ¯èª¤ï¼Œä½¿ç”¨å‚³çµ±æ–¹æ³•...');
        console.log(
          'âš ï¸ [SESSION RAG FALLBACK] Cache failed, falling back to original RAG implementation',
        );

        const queryVector = await generateEmbedding(message, 'query');

        // å„ªå…ˆä½¿ç”¨ Turso å‘é‡æœå°‹
        setStatusText('ğŸŒ æœå°‹çŸ¥è­˜åº« (Turso)...');
        const tursoResults = await searchSimilarChunks(assistantId, queryVector, 50);

        if (tursoResults.length > 0) {
          setStatusText(`ğŸ” å–å¾— ${tursoResults.length} å€‹å€™é¸æ–‡ä»¶...`);
          const topChunks = tursoResults.filter(chunk => chunk.similarity > 0.3);

          // Apply re-ranking to get top 5
          setStatusText('ğŸ”„ é‡æ–°æ’åºç›¸é—œå…§å®¹...');
          const ragChunksForRerank = topChunks.map((chunk, index) => ({
            id: `${chunk.fileName}-${index}`,
            fileName: chunk.fileName,
            content: chunk.content,
            chunkIndex: index,
          }));
          const reRanked = await rerankChunks(message, ragChunksForRerank, 5);

          const contextString = reRanked
            .map(chunk => `From ${chunk.fileName}:\n${chunk.content}`)
            .join('\n\n---\n\n');

          return contextString;
        }

        // å¾Œå‚™ï¼šå¦‚æœ Turso æœå°‹å¤±æ•—ï¼Œä½¿ç”¨æœ¬åœ° ragChunks
        setStatusText('ğŸ“„ æœå°‹æœ¬åœ°çŸ¥è­˜åº«...');
        if (ragChunks.length > 0) {
          const scoredChunks = ragChunks.map(chunk => ({
            ...chunk,
            similarity: chunk.vector ? cosineSimilarity(queryVector, chunk.vector) : 0,
          }));

          scoredChunks.sort((a, b) => b.similarity - a.similarity);
          const topChunks = scoredChunks.slice(0, 5);
          const relevantChunks = topChunks.filter(chunk => chunk.similarity > 0.5);

          const contextString = relevantChunks
            .map(chunk => `From ${chunk.fileName}:\n${chunk.content}`)
            .join('\n\n---\n\n');

          return contextString;
        }

        return '';
      } catch (fallbackError) {
        console.error('âŒ [SESSION RAG FALLBACK] Fallback also failed:', fallbackError);
        setStatusText('âŒ æœå°‹çŸ¥è­˜åº«æ™‚ç™¼ç”ŸéŒ¯èª¤');
        return '';
      }
    }
  };

  const handleSendMessage = async (
    userMessage: string,
    systemPrompt: string,
    assistantId: string,
    ragChunks: RagChunk[],
    setStatusText: (text: string) => void,
    setIsThinking: (thinking: boolean) => void,
    onStreamingChunk: (chunk: string) => void,
    onComplete: (
      tokenInfo: { promptTokenCount: number; candidatesTokenCount: number },
      fullResponse: string,
    ) => void,
    onError: (error: Error) => void,
  ) => {
    // ç«‹å³é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
    const newUserMessage = { role: 'user' as const, content: userMessage };
    const updatedSession = {
      ...currentSession,
      messages: [...currentSession.messages, newUserMessage],
    };
    setCurrentSession(updatedSession);
    onSessionUpdate(updatedSession);

    try {
      setIsThinking(true);
      let ragContext = '';
      ragContext = await findRelevantContext(userMessage, assistantId, ragChunks, setStatusText);

      if (ragContext) {
        setStatusText('ğŸ§  å·²æ“·å–çŸ¥è­˜ã€‚ç”Ÿæˆä¸Šä¸‹æ–‡åŒ–å›ç­”...');
      } else {
        setStatusText('ğŸ¤– ç”Ÿæˆå›ç­”...');
      }

      await streamChat({
        systemPrompt,
        ragContext,
        history: currentSession.messages,
        message: userMessage,
        onChunk: chunk => {
          setIsThinking(false); // ç¬¬ä¸€å€‹ chunk åˆ°é”æ™‚åœæ­¢ thinking å‹•ç•«
          onStreamingChunk(chunk);
        },
        onComplete: (tokenInfo, fullModelResponse) => {
          setIsThinking(false);
          setStatusText('');

          // å‰µå»º AI å›æ‡‰è¨Šæ¯
          const newAiMessage = { role: 'model' as const, content: fullModelResponse };
          const finalSession = {
            ...updatedSession,
            messages: [...updatedSession.messages, newAiMessage],
            tokenCount:
              (updatedSession.tokenCount || 0) +
              tokenInfo.promptTokenCount +
              tokenInfo.candidatesTokenCount,
          };

          setCurrentSession(finalSession);
          onSessionUpdate(finalSession);
          onComplete(tokenInfo, fullModelResponse);
        },
      });
    } catch (error) {
      console.error('Error during chat stream:', error);
      setIsThinking(false);
      setStatusText('');
      onError(error as Error);
    }
  };

  return {
    currentSession,
    handleSendMessage,
    messagesEndRef,
  };
};

export default useSessionManager;
